{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyZP9c6CW7QE"
      },
      "source": [
        "# <a id='toc1_'></a>[Neural Inverted Index for Fast and Effective Information Retrieval](#toc0_)\n",
        "\n",
        "---\n",
        "\n",
        "## <a id='toc1_1_'></a>[üìö Notebook Overview](#toc0_)\n",
        "\n",
        "This notebook explores a novel [information retrieval (IR)](https://en.wikipedia.org/wiki/Information_retrieval) framework that utilizes a **differentiable function** to generate a **sorted list of document identifiers** in response to a given **query**.\n",
        "\n",
        "The approach is called **Differentiable Search Index (DSI)**, and was originally proposed in the paper [Transformer Memory as a Differentiable Search Index](https://arxiv.org/pdf/2202.06991.pdf) by researchers at Google Research.\n",
        "\n",
        "**DSI** aims at both encompassing all document's corpus information and executing retrieval within a single **Transformer language model**, instead of adopting the index-then-retrieve pipeline used in most modern IR sytems.\n",
        "\n",
        "The notebook presents the implemented solution, a **Sequence to Sequence transformer** model `f` that, given a query `q` as input, returns a list of document IDs ranked by relevance to the query, and compares its performance with the traditional **TF-IDF** retrieval model, a **Word2Vec** model, and a **Siamese Network model with Triplet Loss**.\n",
        "\n",
        "The proposed solution combines the **DSI** approach with the **Scheduled Sampling** technique for Transformers, inspired by the similar technique described in the paper [Scheduled Sampling for Transformers](https://arxiv.org/abs/1906.07651).\n",
        "\n",
        "We evaluate the performance of the proposed models using the **Mean Average Precision (MAP)** and the **Recall at K** metrics computed on the **MS MARCO** dataset, and we compare the results with several baselines (**TF-IDF**, **Word2Vec**, **Siamese Network** and also other traditional **Transformer** approaches).\n",
        "\n",
        "## <a id='toc1_2_'></a>[üìù Author](#toc0_)\n",
        "\n",
        "**Valerio Di Stefano** - _\"Sapienza\" University of Rome_\n",
        "<br/>\n",
        "Email: [distefano.1898728@studenti.uniroma1.it](mailto:distefano.1898728@studenti.uniroma1.it)\n",
        "\n",
        "## <a id='toc1_3_'></a>[üîó External Links](#toc0_)\n",
        "\n",
        "* **Main Paper**: [Transformer Memory as a Differentiable Search Index](https://arxiv.org/pdf/2202.06991.pdf)\n",
        "\n",
        "  _Authors_: Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, Tal Schuster, William W. Cohen, Donald Metzler\n",
        "\n",
        "* **Relevant Paper**: [Understanding Differential Search Index for Text Retrieval](https://arxiv.org/abs/2305.02073)\n",
        "\n",
        "  _Authors_: Xiaoyang Chen, Yanjiang Liu, Ben He, Le Sun, Yingfei Sun\n",
        "\n",
        "* **Relevant Paper**: [Scheduled Sampling for Transformers](https://arxiv.org/abs/1906.07651)\n",
        "\n",
        "    _Authors_: Tsvetomila Mihaylova, Andr√© F. T. Martins\n",
        "\n",
        "* **Project Repository**: [GitHub Repository](https://github.com/valeriodiste/deep_learning_project)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zL3Fr68W7QG"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWBsWjvVW7QG"
      },
      "source": [
        "\n",
        "## <a id='toc1_4_'></a>[üìå Table of Contents](#toc0_)\n",
        "\n",
        "**Table of contents**<a id='toc0_'></a>    \n",
        "\n",
        "- [Neural Inverted Index for Fast and Effective Information Retrieval](#toc1_)    \n",
        "  - [üìö Notebook Overview](#toc1_1_)    \n",
        "  - [üìù Author](#toc1_2_)    \n",
        "  - [üîó External Links](#toc1_3_)    \n",
        "  - [üìå Table of Contents](#toc1_4_)    \n",
        "\n",
        "- [üöÄ Getting Started](#toc2_)    \n",
        "  - [Collect Source Files](#toc2_1_)    \n",
        "  - [Install & Import Libraries](#toc2_2_)    \n",
        "  - [Configuration, Hyperparameters and Constants](#toc2_3_)    \n",
        "  - [Download Data & Resources](#toc2_4_)    \n",
        "\n",
        "- [üíæ Data Preparation](#toc3_)    \n",
        "  - [Word2Vec Model Initialization](#toc3_2_)    \n",
        "  - [Dictionaries Creation & Word2Vec Model Training](#toc3_3_)    \n",
        "  - [Dictionaries Loading](#toc3_4_)    \n",
        "\n",
        "- [üßæ TF-IDF Model](#toc4_)    \n",
        "\n",
        "- [üîù Word2Vec Model](#toc5_)    \n",
        "\n",
        "- [üë¨ Siamese Network with Triplet Loss](#toc6_)    \n",
        "\n",
        "- [ü§ñ Seq2Seq Transformer Model (DSI approach)](#toc7_)    \n",
        "  - [Teacher Forcing Seq2Seq Transformer Model](#toc7_2_)    \n",
        "  - [Autoregressive Seq2Seq Transformer Model](#toc7_3_)    \n",
        "  - [Scheduled Sampling Seq2Seq Transformer Model](#toc7_4_)    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGdCNyGIW7QH"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrWHof7WW7QH"
      },
      "source": [
        "\n",
        "\n",
        "<a id=\"1\"></a>\n",
        "# <a id='toc2_'></a>[üöÄ Getting Started](#toc0_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KwLmmxOW7QH"
      },
      "source": [
        "First of all, we check if we are running the notebook on Google colab or locally, defining the `RUNNING_ON_COLAB` constant used throughout the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnFvNpE-W7QH"
      },
      "outputs": [],
      "source": [
        "# Check if running on colab or locally\n",
        "try:\n",
        "    from google.colab import files\n",
        "    RUNNING_IN_COLAB = True\n",
        "    print(\"Running on Google Colab.\")\n",
        "except ModuleNotFoundError:\n",
        "    RUNNING_IN_COLAB = False\n",
        "    print(\"Running locally.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOoNN4ybW7QI"
      },
      "source": [
        "\n",
        "<a id=\"1_1\"></a>\n",
        "## <a id='toc2_1_'></a>[Collect Source Files](#toc0_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e95zfPDJW7QI"
      },
      "source": [
        "#### <a id='toc2_1_1_1_'></a>[Clone Project's GitHub Repository](#toc0_)\n",
        "\n",
        "We **clone the project's repository** from GitHub to access the source files for datasets, models, evaluation and utilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hK7QNbYW7QI"
      },
      "outputs": [],
      "source": [
        "# Clone the git repository from \"https://github.com/valeriodiste/deep_learning_project\" (for the source files)\n",
        "!git clone https://github.com/valeriodiste/deep_learning_project.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6-V-PGqW7QI"
      },
      "source": [
        "#### <a id='toc2_1_1_2_'></a>[Pull Latest Files Changes](#toc0_)\n",
        "\n",
        "We also **pull the latest changes** from the repository and store them in the `./deep_learning_project` directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTO-K52dW7QI"
      },
      "outputs": [],
      "source": [
        "# Change the working directory to the cloned repository\n",
        "%cd /content/deep_learning_project\n",
        "# Pull the latest changes from the repository\n",
        "!git pull origin main\n",
        "# Change the working directory to the parent directory\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4KCoH1UW7QI"
      },
      "source": [
        "<a id=\"1_2\"></a>\n",
        "## <a id='toc2_2_'></a>[Install & Import Libraries](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdvMf7SQW7QI"
      },
      "source": [
        "#### <a id='toc2_2_1_1_'></a>[Install Libraries](#toc0_)\n",
        "\n",
        "We **install all the necessary libraries** for this notebook.\n",
        "\n",
        "- **`pytorch-lightning`**: A **lightweight PyTorch wrapper** for simplifying PyTorch code.\n",
        "- **`ir_datasets`**: A Python library for accessing **information retrieval datasets** (used to load the **\"MS MARCO\" dataset**).\n",
        "- **`wandb`**: The python package for **Weights & Biases**, a tool for experiment tracking, dataset versioning, and project collaboration (used for **logging and visualization**)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AgkA542W7QI"
      },
      "outputs": [],
      "source": [
        "# Install the required packages\n",
        "%%capture\n",
        "%pip install pytorch-lightning\n",
        "%pip install ir_datasets\n",
        "%pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrDv0niAW7QI"
      },
      "source": [
        "#### <a id='toc2_2_1_2_'></a>[Import Modules](#toc0_)\n",
        "\n",
        "We then **import the required modules**, including `PyTorch`, `PyTorch Lightning`, `IR Datasets` and `W&B`, plus other useful modules and libraries (`NLTK`, `Scikit Learn`, `Numpy`, `Pandas`, etc...)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYzvoExIW7QJ"
      },
      "outputs": [],
      "source": [
        "# Import the standard libraries\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "import math\n",
        "\n",
        "# Import the PyTorch libraries and modules\n",
        "import torch\n",
        "\n",
        "# Import the PyTorch Lightning libraries and modules\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "# Import the ir_datasets\n",
        "import ir_datasets\n",
        "\n",
        "# Import the W&B (Weights & Biases) library\n",
        "import wandb\n",
        "from wandb.sdk import wandb_run\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "# Import the scikit-learn TF-IDF vectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Import the NLTK (Natural Language Toolkit) library\n",
        "import nltk\n",
        "\n",
        "# Import the tqdm library (for the progress bars)\n",
        "if not RUNNING_IN_COLAB:\n",
        "    from tqdm import tqdm\n",
        "else:\n",
        "    from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS7SbDJlW7QJ"
      },
      "source": [
        "We also import our own **custom modules** (cloned from the repository) containing Python classes for **datasets**, **models**, **evaluation**, and **utilities**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0Zq6ypFW7QJ"
      },
      "outputs": [],
      "source": [
        "# Import the custom modules\n",
        "if not RUNNING_IN_COLAB:\n",
        "    # We are running locally (not on Google Colab, import modules from the \"src\" directory in the current directory)\n",
        "    from src.scripts import models, datasets, training, evaluation\n",
        "    from src.scripts.utils import (\n",
        "        print_json, MODEL_TYPES, RANDOM_SEED, MODEL_CHECKPOINTS_FILES, get_preprocessed_text, print_model_evaluation_results\n",
        "    )\n",
        "else:\n",
        "    # We are running on Google Colab (import modules from the pulled repository stored in the \"deep_learning_project\" directory)\n",
        "    from deep_learning_project.src.scripts import models, datasets, training, evaluation\n",
        "    from deep_learning_project.src.scripts.utils import (\n",
        "        print_json, MODEL_TYPES, RANDOM_SEED, MODEL_CHECKPOINTS_FILES, get_preprocessed_text, print_model_evaluation_results\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIfvM5HPW7QJ"
      },
      "source": [
        "<a id=\"1_3\"></a>\n",
        "## <a id='toc2_3_'></a>[Configuration, Hyperparameters and Constants](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZ_atBaWW7QJ"
      },
      "source": [
        "#### <a id='toc2_3_1_1_'></a>[Random Seed](#toc0_)\n",
        "\n",
        "We **seed the random number generators** for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6-mmMEmW7QJ"
      },
      "outputs": [],
      "source": [
        "# Set the random seeds for reproducibility\n",
        "random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "pl.seed_everything(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTV_XfxiW7QJ"
      },
      "source": [
        "#### <a id='toc2_3_1_2_'></a>[Device Configuration](#toc0_)\n",
        "\n",
        "We **set the device** to GPU if available, otherwise we use the CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cgGByHAW7QJ"
      },
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device.type}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkduSUJsW7QJ"
      },
      "source": [
        "#### <a id='toc2_3_1_3_'></a>[Database Constants](#toc0_)\n",
        "\n",
        "We **define the constants** used for the **database resources download** and the **dataset creation**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-mNAOE_W7QJ"
      },
      "outputs": [],
      "source": [
        "# Define the max number of queries of the dataset (note that the MS MARCO dataset used contains 6980 queries, numbers higher than this will be ignored)\n",
        "#   Set to -1 to use all the available queries in the MS MARCO dataset used\n",
        "#   NOTE: this will also indirectly influence the number of documents in the final dataset, as only documents that are relevant to at least one of the selected queries will be kept\n",
        "MAX_DATASET_QUERIES = 1_000\n",
        "\n",
        "# Set the number of relevant documents associated to each query (when \"scoreddocs\" are used, a maximum value of 1_000 documents can be used)\n",
        "#   Set to -1 to use all the available relevant documents for each query in the MS MARCO dataset used\n",
        "#   NOTE: the actual number of relevant documents for some queries might be higher than this value, since the final documents dataset will include all documents\n",
        "#       associated to at least one query, and some queries might be relevant to their own set of documents plus some documents relevant to other queries\n",
        "#       (which will still be added to the list, thus exceeding the defined NUMBER_OF_RELEVANT_DOCUMENTS_PER_QUERY in these cases)\n",
        "NUMBER_OF_RELEVANT_DOCUMENTS_PER_QUERY = 10\n",
        "\n",
        "# Wheter to remap doc IDs to new IDs (starting from 0 up until the number of documents in the final documents dataset)\n",
        "REMAP_DOC_IDS = True\n",
        "\n",
        "# Defines wheter to use the MS MARCO documents dataset (very heavy) or the MS MARCO passages dataset (smaller and faster to download and process)\n",
        "USE_DOCUMENTS_DATASETS = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KF2_I_fUW7QK"
      },
      "source": [
        "#### <a id='toc2_3_1_4_'></a>[Models Hyperparameters](#toc0_)\n",
        "\n",
        "We then **define the constant** representing **hyperparameters** used for the **Word2Vec model**, the **Siamese Network model** and for the **Seq2Seq transformer model**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVuO8Fe7W7QK"
      },
      "outputs": [],
      "source": [
        "# Define the max length of the embeddings for both queries and documents for the Word2Vec model\n",
        "VECTOR_EMBEDDINGS_SIZE = 128\n",
        "\n",
        "# Define the size of the output vector embeddings of the Siamese network model\n",
        "SIAMESE_EMBEDDINGS_SIZE = 64\n",
        "\n",
        "# Define the max length of the tokenized queries and documents for the Transformer model (embeddings will be padded or truncated to this length)\n",
        "TRANSFORMER_DOCUMENT_MAX_TOKENS = 64\n",
        "TRANSFORMER_QUERY_MAX_TOKENS = 32\n",
        "\n",
        "# Define the size of the embeddings for the Encoders of the Seq2Seq Transformer model\n",
        "TRANSFORMER_EMBEDDINGS_SIZE = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BnkBVF1W7QK"
      },
      "source": [
        "#### <a id='toc2_3_1_5_'></a>[Evauation Constants](#toc0_)\n",
        "\n",
        "We also define the constants used for the evaluation of the various models (i.e. to compute the **Mean Average Precision** and the **Recall at K**)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3z312t5W7QK"
      },
      "outputs": [],
      "source": [
        "# Define the number of documents K to retrieve for each query and the number of queries N to calculate the mean average precision (MAP@K)\n",
        "MAP_K = 10\n",
        "MAP_N = 10\n",
        "\n",
        "# Define the number of documents K to retrieve for each query to calculate the Recall@K metrics\n",
        "RECALL_K = 1_000\n",
        "\n",
        "# Whether to print the debug information during the MAP@K and Recall@K evaluation of the models\n",
        "PRINT_EVALUATION_DEBUG = True\n",
        "\n",
        "# Whether to evaluate the models (i.e. compute the MAP@K and Recall@K metrics for the trained models on the test datasets)\n",
        "EVALUATE_MODELS = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oN5KBop6W7QK"
      },
      "source": [
        "#### <a id='toc2_3_1_6_'></a>[Other Constants](#toc0_)\n",
        "\n",
        "We ultimately define the constants used to determine where to save data and models and the flags to enable/disable database rebuild/refresh and the loading of models checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFo3Xe4MW7QK"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the data folder, onto which the documents and queries dictionaries will be saved\n",
        "DATA_FOLDER = \"src/data\" if not RUNNING_IN_COLAB else \"/content/data\"\n",
        "\n",
        "# Define the path to save models\n",
        "MODELS_FOLDER = \"src/models\" if not RUNNING_IN_COLAB else \"/content/models\"\n",
        "\n",
        "# Force the rebuild of the documents and queries dictionaries (to re-save them to the JSON files)\n",
        "FORCE_DICTIONARIES_REBUILD = False\n",
        "\n",
        "# Refreshes the embeddings of the documents and queries (if set to True, the embeddings will be recomputed and saved to the JSON files, used to change properties of the embeddings, e.g. the EMBEDDINGS_SIZE, without having to rebuild the dictionaries)\n",
        "REFRESH_EMBEDDINGS = False\n",
        "\n",
        "# Whether to load model checkpoints (if they were already saved locally) or not\n",
        "LOAD_MODELS_CHECKPOINTS = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fwooArSW7QK"
      },
      "source": [
        "#### <a id='toc2_3_1_7_'></a>[Local Files Folder Creation](#toc0_)\n",
        "\n",
        "We create the folders to store the data dictionaries and the model's checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Chyt_ybzW7QK"
      },
      "outputs": [],
      "source": [
        "# Create folders if they do not exist\n",
        "if not os.path.exists(DATA_FOLDER):\n",
        "    print(f\"Creating the data folder at '{DATA_FOLDER}'...\")\n",
        "    os.makedirs(DATA_FOLDER)\n",
        "if not os.path.exists(MODELS_FOLDER):\n",
        "    print(f\"Creating the models folder at '{MODELS_FOLDER}'...\")\n",
        "    os.makedirs(MODELS_FOLDER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8keIFsmvW7QK"
      },
      "source": [
        "#### <a id='toc2_3_1_8_'></a>[Weights & Biases Configuration](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9_xNrpcW7QL"
      },
      "source": [
        "We set the **Weights & Biases** API key to log the experiments.\n",
        "\n",
        "**‚ö†Ô∏è Note**: Copy and paste your own W&B API key into the `WANDB_API_KEY` constant to see logging results, or set the constant to an empty string to disable W&B logging (this won't plot training losses and accuracies over time)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVM3l8IcW7QL"
      },
      "outputs": [],
      "source": [
        "# Define the WANDB_API_KEY (set to \"\" to disable W&B logging)\n",
        "WANDB_API_KEY = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1EYR8RRW7QL"
      },
      "source": [
        "We configure the **Weights & Biases** logger and API to track the experiments and the model's performances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVXlI87xW7QL"
      },
      "outputs": [],
      "source": [
        "# Define the wandb logger, api object, entity name and project name\n",
        "wandb_logger = None\n",
        "wandb_api = None\n",
        "wandb_entity = None\n",
        "wandb_project = None\n",
        "# Check if a W&B api key is provided\n",
        "if WANDB_API_KEY == None:\n",
        "    print(\"No W&B API key provided, please provide a valid key to use the W&B API or set the WANDB_API_KEY variable to an empty string to disable logging\")\n",
        "    raise ValueError(\"No W&B API key provided.\")\n",
        "elif WANDB_API_KEY != \"\":\n",
        "    # Login to the W&B (Weights & Biases) API\n",
        "    wandb.login(key=WANDB_API_KEY, relogin=True)\n",
        "    # Minimize the logging from the W&B (Weights & Biases) library\n",
        "    os.environ[\"WANDB_SILENT\"] = \"true\"\n",
        "    logging.getLogger(\"wandb\").setLevel(logging.ERROR)\n",
        "    # Initialize the W&B (Weights & Biases) loggger\n",
        "    wandb_logger = WandbLogger(\n",
        "        log_model=\"all\", project=\"dl-dsi-project\", name=\"- SEPARATOR -\")\n",
        "    # Initialize the W&B (Weights & Biases) API\n",
        "    wandb_api = wandb.Api()\n",
        "    # Get the W&B (Weights & Biases) entity name\n",
        "    wandb_entity = wandb_logger.experiment.entity\n",
        "    # Get the W&B (Weights & Biases) project name\n",
        "    wandb_project = wandb_logger.experiment.project\n",
        "    # Finish the \"separator\" experiment\n",
        "    wandb_logger.experiment.finish(quiet=True)\n",
        "    print(\"W&B API key provided, logging with W&B enabled.\")\n",
        "else:\n",
        "    print(\"No W&B API key provided, logging with W&B disabled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvW_0Nz2W7QL"
      },
      "source": [
        "<a id=\"1_4\"></a>\n",
        "## <a id='toc2_4_'></a>[Download Data & Resources](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCk7jhQEW7QL"
      },
      "source": [
        "#### <a id='toc2_4_1_1_'></a>[Download NLTK Resources](#toc0_)\n",
        "\n",
        "We download the needed NLTK resources for text preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nat10cRsW7QL"
      },
      "outputs": [],
      "source": [
        "# Download the needed NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XYWTLlbW7QL"
      },
      "source": [
        "#### <a id='toc2_4_1_2_'></a>[Download MS MARCO Dataset](#toc0_)\n",
        "\n",
        "Download the **MS MARCO** dataset's resources for the `ir_dataset` module (if needed).\n",
        "\n",
        "The `USE_DOCUMENTS_DATASETS` flag is used to determine whether to download the \"documents\" version of the dataset or its \"passages\" version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cABdiHNGW7QM"
      },
      "outputs": [],
      "source": [
        "# Download the MS MARCO dataset (if needed and if the dictionaries need to be built/rebuilt)\n",
        "if FORCE_DICTIONARIES_REBUILD or not os.path.exists(DATA_FOLDER + \"/docs_dict.json\") or not os.path.exists(DATA_FOLDER + \"/queries_dict.json\"):\n",
        "\n",
        "    # Load the MS MARCO dataset\n",
        "    dataset = None\n",
        "    if USE_DOCUMENTS_DATASETS:\n",
        "        # Load https://ir-datasets.com/msmarco-passage.html#msmarco-document/dev\n",
        "        dataset = ir_datasets.load(\"msmarco-document/dev\")\n",
        "    else:\n",
        "        # Load https://ir-datasets.com/msmarco-passage.html#msmarco-passage/dev/small\n",
        "        dataset = ir_datasets.load(\"msmarco-passage/dev/small\")\n",
        "\n",
        "    # Triggers the download of the datasets (if not already downloaded)\n",
        "    dataset.docs_iter().__next__()\n",
        "    dataset.queries_iter().__next__()\n",
        "    dataset.qrels_iter().__next__()\n",
        "    dataset.scoreddocs_iter().__next__()\n",
        "\n",
        "    # Print the dataset structure (i.e. the column names)\n",
        "    print_metadata = False\n",
        "    if print_metadata:\n",
        "        print(\"Docs Metadata:\")\n",
        "        print_json(dataset.docs_metadata(), 2)\n",
        "        print(\"Queries Metadata:\")\n",
        "        print_json(dataset.queries_metadata(), 2)\n",
        "        print(\"Qrels Metadata:\")\n",
        "        print_json(dataset.qrels_metadata(), 2)\n",
        "        print(\"Scored Docs Metadata:\")\n",
        "        print_json(dataset.scoreddocs_metadata(), 2)\n",
        "\n",
        "    # Print some samples of the dataset\n",
        "    print_database_samples = False\n",
        "    if print_database_samples:\n",
        "        # Print a sample document\n",
        "        print(\"\\nSample Document:\")\n",
        "        print(\"  <doc_id, url, title, body>\"\n",
        "              if USE_DOCUMENTS_DATASETS\n",
        "              else \"  <doc_id, text>\")\n",
        "        doc = dataset.docs_iter().__next__()\n",
        "        print_json(doc, 2)\n",
        "        # Print a sample query\n",
        "        print(\"\\nSample Query:\")\n",
        "        print(\"  <query_id, text>\")\n",
        "        query = dataset.queries_iter().__next__()\n",
        "        print_json(query, 2)\n",
        "        # Print a sample qrel\n",
        "        #   NOTE: the \"relevance\" and \"iteration\" fields are always 1 and \"0\" respectively, for all the qrels (qrels only contain relevant pairs of <query_id, doc_id>)\n",
        "        print(\"\\nSample Qrel:\")\n",
        "        print(\"  <query_id, doc_id, relevance, iteration>\")\n",
        "        qrel = dataset.qrels_iter().__next__()\n",
        "        print_json(qrel, 2)\n",
        "        # Print a sample scored doc\n",
        "        print(\"\\nSample Scored Doc:\")\n",
        "        print(\"  <query_id, doc_id, score>\")\n",
        "        scored_doc = dataset.scoreddocs_iter().__next__()\n",
        "        print_json(scored_doc, 2)\n",
        "else:\n",
        "    # Print a message indicating that the dictionaries already exist and will be loaded\n",
        "    print(\"No need to download the MS MARCO dataset.\")\n",
        "    print(\"Documents and queries dictionaries already exist and will be loaded from the JSON files.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK82SdaDW7QM"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39VUQZvZW7QM"
      },
      "source": [
        "\n",
        "<a id=\"2\"></a>\n",
        "# <a id='toc3_'></a>[üíæ Data Preparation](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqOVts9BW7QM"
      },
      "source": [
        "#### <a id='toc3_1_1_1_'></a>[Data Variables and Constants](#toc0_)\n",
        "\n",
        "We define the `docs_dict` and `queries_dict` dictionaries used to store the documents and queries data.\n",
        "\n",
        "The `docs_dict` dictionary contains, for each document ID, the documents' text and its Word2Vec embedding.\n",
        "\n",
        "The `queries_dict` dictionary contains, for each query ID, the query's text, its Word2Vec embedding and also the list of document IDs for documents relevant to the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJypxBvsW7QM"
      },
      "outputs": [],
      "source": [
        "# Dictionaries to store the documents and queries (the main dataset)\n",
        "docs_dict = {}\n",
        "queries_dict = {}\n",
        "\n",
        "# Auxiliary dictionary to map the column names to the corresponding index in the ir_datasets tuples\n",
        "IR_DATASET_COLS = {\n",
        "    \"DOCS\": {\"id\": 0, \"url\": 1, \"title\": 2, \"body\": 3} if USE_DOCUMENTS_DATASETS else {\"id\": 0, \"text\": 1},\n",
        "    \"QUERIES\": {\"id\": 0, \"text\": 1},\n",
        "    \"QRELS\": {\"query_id\": 0, \"doc_id\": 1},\n",
        "    \"SCORED_DOCS\": {\"query_id\": 0, \"doc_id\": 1, \"score\": 2}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj1RbSj9W7QM"
      },
      "source": [
        "<a id=\"2_1\"></a>\n",
        "## <a id='toc3_2_'></a>[Word2Vec Model Initialization](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaYMRsxLW7QM"
      },
      "source": [
        "We initialize the `Word2Vec` model to compute the **vector embeddings** of the documents and queries.\n",
        "\n",
        "This model is later **trained on the documents corpus** (using the `Gensim` library) to output vector embeddings of size `VECTOR_EMBEDDINGS_SIZE` for documents and queries.\n",
        "\n",
        "This model is also used as a **baseline** for the evaluation of the final **Seq2Seq** transformer model (we compute the cosine similarity of the output embeddings between a query and the entire documents database to generate the top `K` most relevant documents)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UzNkQReW7QM"
      },
      "outputs": [],
      "source": [
        "# Initialize a Word2Vec model to encode the text\n",
        "word2vec_model = models.Word2VecModel(\n",
        "    embeddings_size=VECTOR_EMBEDDINGS_SIZE,\n",
        "    words_window_size=10,\n",
        "    min_word_frequency=0,\n",
        "    learning_rate=0.025,\n",
        "    max_epochs=20,\n",
        "    save_path=MODELS_FOLDER + \"/\" +\n",
        "    MODEL_CHECKPOINTS_FILES[MODEL_TYPES.WORD2VEC]\n",
        ")\n",
        "\n",
        "\n",
        "def load_or_train_word2vec_model(documents_corpus=None):\n",
        "    '''\n",
        "    Train the word2vec model if the checkpoint file does not exist, otherwise load the model from the checkpoint file\n",
        "\n",
        "    If document_corpus is None or is an empty list, a new document corpus will be created using the documents in the dataset\n",
        "\n",
        "    If a document_corpus is provided (as a list of list of strings representing the words of each document's text), it will be used to train the Word2Vec model\n",
        "    '''\n",
        "    loaded_checkpoint = False\n",
        "    if LOAD_MODELS_CHECKPOINTS:\n",
        "        loaded_checkpoint = word2vec_model.load()\n",
        "    if not loaded_checkpoint:\n",
        "        # Train the Word2Vec model on the documents corpus\n",
        "        print(\"Training the Word2Vec model on the documents corpus...\")\n",
        "        if documents_corpus is None or len(documents_corpus) == 0:\n",
        "            # Build the documents corpus\n",
        "            documents_corpus = [get_preprocessed_text(\n",
        "                docs_dict[doc_id][\"text\"]).split(\" \") for doc_id in docs_dict]\n",
        "        # Train the Word2Vec model\n",
        "        word2vec_model.train(documents_corpus)\n",
        "        print(\"Word2Vec model training completed.\")\n",
        "    else:\n",
        "        print(\"Word2Vec model loaded from the checkpoint file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFHSoY23W7QN"
      },
      "source": [
        "<a id=\"2_2\"></a>\n",
        "\n",
        "## <a id='toc3_3_'></a>[Dictionaries Creation & Word2Vec Model Training](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfYxybh_W7QN"
      },
      "source": [
        "We build the **documents** and **queries** dictionaries if needed (or if the `FORCE_DICTIONARIES_REBUILD` flag is set to `True`).\n",
        "\n",
        "Documents and queries **vector embeddings** are also created, using the `Word2Vec` model trained on the documents' corpus.\n",
        "\n",
        "If the `REMAP_DOC_IDS` flag is set to `True`, document IDs are also **remapped to new IDs** to avoid gaps in the dictionary.\n",
        "\n",
        "We ultimately **save the dictionaries** into local files in the `DATA_FOLDER` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL166GSGW7QN"
      },
      "outputs": [],
      "source": [
        "# Check if the dictionaries need to be built/rebuilt\n",
        "if FORCE_DICTIONARIES_REBUILD or not os.path.exists(DATA_FOLDER + \"/docs_dict.json\") or not os.path.exists(DATA_FOLDER + \"/queries_dict.json\"):\n",
        "\n",
        "    print(\"The documents and queries dictionaries files do not exist, creating them...\")\n",
        "\n",
        "    # Build a queries dictionary, containing the query_id as key, and as values both the query text and a list of associated relevant documents (as doc_id) taken from the scored documents (list of the 1000 relevant documents to the query)\n",
        "    number_of_queries = MAX_DATASET_QUERIES \\\n",
        "        if 0 < MAX_DATASET_QUERIES < dataset.queries_count() \\\n",
        "        else dataset.queries_count()\n",
        "    use_scored_docs_for_relevant_documents = True\n",
        "    for query in tqdm(dataset.queries_iter(), \"Building the queries dictionary\", number_of_queries):\n",
        "        if len(queries_dict) >= number_of_queries:\n",
        "            break\n",
        "        query_id = query[IR_DATASET_COLS[\"QUERIES\"][\"id\"]]\n",
        "        query_text = query[IR_DATASET_COLS[\"QUERIES\"][\"text\"]]\n",
        "        queries_dict[query_id] = {\n",
        "            \"text\": query_text,\n",
        "            \"embedding\": None,\n",
        "            \"relevant_docs\": []\n",
        "        }\n",
        "    # Add the relevant documents to the queries dictionary\n",
        "    doc_ids_with_rel = set()\n",
        "    # First, add the relevant document(s) using the qrels (to ensure the most relevant documents are added first)\n",
        "    for qrel in tqdm(dataset.qrels_iter(), \"Adding relevant documents to queries (using qrels)\", dataset.qrels_count()):\n",
        "        query_id = qrel[IR_DATASET_COLS[\"QRELS\"][\"query_id\"]]\n",
        "        if query_id not in queries_dict:\n",
        "            continue\n",
        "        doc_id = qrel[IR_DATASET_COLS[\"QRELS\"][\"doc_id\"]]\n",
        "        if NUMBER_OF_RELEVANT_DOCUMENTS_PER_QUERY < 0 or len(queries_dict[query_id][\"relevant_docs\"]) < NUMBER_OF_RELEVANT_DOCUMENTS_PER_QUERY:\n",
        "            if doc_id not in queries_dict[query_id][\"relevant_docs\"]:\n",
        "                queries_dict[query_id][\"relevant_docs\"].append(doc_id)\n",
        "                doc_ids_with_rel.add(doc_id)\n",
        "    # Then, add the relevant documents using the scoreddocs (if needed)\n",
        "    # NOTE: the scoreddocs list contains 1000 relevant documents to the query, unordered and without an associated relevance score (these results are less precise than the qrels)\n",
        "    if use_scored_docs_for_relevant_documents:\n",
        "        for scored_doc in tqdm(dataset.scoreddocs_iter(), \"Adding relevant documents to queries (using scoreddocs)\", dataset.scoreddocs_count()):\n",
        "            query_id = scored_doc[IR_DATASET_COLS[\"SCORED_DOCS\"][\"query_id\"]]\n",
        "            if query_id not in queries_dict:\n",
        "                continue\n",
        "            doc_id = scored_doc[IR_DATASET_COLS[\"SCORED_DOCS\"][\"doc_id\"]]\n",
        "            if NUMBER_OF_RELEVANT_DOCUMENTS_PER_QUERY < 0 or len(queries_dict[query_id][\"relevant_docs\"]) < NUMBER_OF_RELEVANT_DOCUMENTS_PER_QUERY:\n",
        "                if doc_id not in queries_dict[query_id][\"relevant_docs\"]:\n",
        "                    queries_dict[query_id][\"relevant_docs\"].append(doc_id)\n",
        "                    doc_ids_with_rel.add(doc_id)\n",
        "        # Fix the missing relevant documents from the queries dictionary (if the relevant documents list was reduced)\n",
        "        if NUMBER_OF_RELEVANT_DOCUMENTS_PER_QUERY > 0:\n",
        "            # Re-add to the relevant documents list of each query all the removed documents that will be added to the documents dataset (i.e. in the doc_ids_with_rel set)\n",
        "            for scored_doc in tqdm(dataset.scoreddocs_iter(), \"Fixing missing relevant documents from queries dictionary\", dataset.scoreddocs_count()):\n",
        "                query_id = scored_doc[IR_DATASET_COLS[\"SCORED_DOCS\"][\"query_id\"]]\n",
        "                if query_id not in queries_dict:\n",
        "                    continue\n",
        "                doc_id = scored_doc[IR_DATASET_COLS[\"SCORED_DOCS\"][\"doc_id\"]]\n",
        "                if doc_id in doc_ids_with_rel and doc_id not in queries_dict[query_id][\"relevant_docs\"]:\n",
        "                    queries_dict[query_id][\"relevant_docs\"].append(doc_id)\n",
        "    print(\n",
        "        f\"Total number of documents relevant to at least one query: {len(doc_ids_with_rel)}\")\n",
        "\n",
        "    # Initialize the corpus of documents (to be used to train the Word2Vec model)\n",
        "    documents_corpus = []\n",
        "\n",
        "    # Build a documents dictionary, containing the doc_id as key, and the attribute \"text\" containing the document text\n",
        "    documents_count = 0\n",
        "    documents_id_remapping = {}\n",
        "    documents_id_remapping_inverse = {}\n",
        "    for doc in tqdm(dataset.docs_iter(), \"Building the documents dictionary\", dataset.docs_count()):\n",
        "        # Add the document and its text to the documents dictionary\n",
        "        doc_id = doc[IR_DATASET_COLS[\"DOCS\"][\"id\"]]\n",
        "        if doc_id not in doc_ids_with_rel:\n",
        "            continue\n",
        "        doc_text = \"\"\n",
        "        if USE_DOCUMENTS_DATASETS:\n",
        "            doc_text = doc[IR_DATASET_COLS[\"DOCS\"][\"title\"]] + \\\n",
        "                \".\\n\" + doc[IR_DATASET_COLS[\"DOCS\"][\"body\"]]\n",
        "        else:\n",
        "            doc_text = doc[IR_DATASET_COLS[\"DOCS\"][\"text\"]]\n",
        "        docs_dict[doc_id] = {\n",
        "            \"text\": doc_text,\n",
        "            \"embedding\": None\n",
        "        }\n",
        "        # Compute the remapped doc_id (if needed)\n",
        "        if REMAP_DOC_IDS:\n",
        "            new_doc_id = str(documents_count)\n",
        "            documents_id_remapping[doc_id] = new_doc_id\n",
        "            documents_id_remapping_inverse[new_doc_id] = doc_id\n",
        "        # Increment the documents count\n",
        "        documents_count += 1\n",
        "        # Add the document text to the corpus\n",
        "        documents_corpus.append(get_preprocessed_text(doc_text).split(\" \"))\n",
        "\n",
        "    # Load or train the Word2Vec model on the documents corpus\n",
        "    load_or_train_word2vec_model(documents_corpus)\n",
        "\n",
        "    # Scramble the documents and queries dictionaries\n",
        "    if REMAP_DOC_IDS:\n",
        "        print(\"Scrambling the documents and queries dictionaries...\")\n",
        "        docs_dict = dict(random.sample(docs_dict.items(), len(docs_dict)))\n",
        "        queries_dict = dict(random.sample(queries_dict.items(), len(queries_dict)))\n",
        "        print(\"Scrambled the documents and queries dictionaries.\")\n",
        "\n",
        "    # Iterate over documents in the dictionaries to compute the embeddings (and to eventually remap the doc_ids)\n",
        "    new_docs_dict = {}\n",
        "    for doc_id in tqdm(docs_dict, \"Computing document embeddings\" + (\" and remapping doc_ids\" if REMAP_DOC_IDS else \"\")):\n",
        "        # Compute the embedding of the document text\n",
        "        docs_dict[doc_id][\"embedding\"] = \\\n",
        "            word2vec_model.get_embedding((docs_dict[doc_id][\"text\"]))\n",
        "        # Remap the doc_id (if needed)\n",
        "        if REMAP_DOC_IDS:\n",
        "            new_docs_dict[documents_id_remapping[doc_id]] = {\n",
        "                \"text\": docs_dict[doc_id][\"text\"],\n",
        "                \"embedding\": docs_dict[doc_id][\"embedding\"]\n",
        "            }\n",
        "    if REMAP_DOC_IDS:\n",
        "        docs_dict = new_docs_dict\n",
        "    # Iterate over queries in the dictionary to compute the embeddings (and to eventually remap the relevant doc_ids)\n",
        "    for query_id in tqdm(queries_dict, \"Computing query embeddings\" + (\" and remapping relevant doc_ids\" if REMAP_DOC_IDS else \"\")):\n",
        "        # Compute the embedding of the query text\n",
        "        queries_dict[query_id][\"embedding\"] = \\\n",
        "            word2vec_model.get_embedding(queries_dict[query_id][\"text\"])\n",
        "        # Remap the relevant documents (if needed)\n",
        "        if REMAP_DOC_IDS:\n",
        "            current_relevant_docs = queries_dict[query_id][\"relevant_docs\"]\n",
        "            queries_dict[query_id][\"relevant_docs\"] = [\n",
        "                documents_id_remapping[doc_id] for doc_id in current_relevant_docs]\n",
        "\n",
        "    # Print the total number of documents and queries\n",
        "    print(f\"Total number of documents (in built dict): {len(docs_dict)}\")\n",
        "    print(f\"Total number of queries (in built dict): {len(queries_dict)}\")\n",
        "\n",
        "    # Save the 2 dictionaries to 2 JSON files in the \"data\" directory\n",
        "    print(\"Saving the documents and queries dictionaries to the JSON files...\")\n",
        "    with open(DATA_FOLDER + \"/docs_dict.json\", \"w\") as docs_dict_file:\n",
        "        json.dump(docs_dict, docs_dict_file, indent=2)\n",
        "    with open(DATA_FOLDER + \"/queries_dict.json\", \"w\") as queries_dict_file:\n",
        "        json.dump(queries_dict, queries_dict_file, indent=2)\n",
        "    print(\"Created the documents and queries dictionaries and saved them to the files.\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxadKj9MW7QN"
      },
      "source": [
        "<a id=\"2_3\"></a>\n",
        "\n",
        "## <a id='toc3_4_'></a>[Dictionaries Loading](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rsjYcc4W7QN"
      },
      "source": [
        "We load the `documents` and `queries` dictionaries from the local files in the `DATA_FOLDER` dicectory and save them to the corresponding dictionary variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RF1PLq9OW7QN"
      },
      "outputs": [],
      "source": [
        "# Load the documents and queries dictionaries from the JSON files\n",
        "print(\"Loading the documents and queries dictionaries from the files...\")\n",
        "with open(DATA_FOLDER + \"/docs_dict.json\", \"r\") as docs_dict_file:\n",
        "    docs_dict = json.load(docs_dict_file)\n",
        "print(f\"  Loaded {len(docs_dict)} documents\")\n",
        "with open(DATA_FOLDER + \"/queries_dict.json\", \"r\") as queries_dict_file:\n",
        "    queries_dict = json.load(queries_dict_file)\n",
        "print(f\"  Loaded {len(queries_dict)} queries\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbIemImRW7QO"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP_hAVnOW7QO"
      },
      "source": [
        "<a id=\"3\"></a>\n",
        "# <a id='toc4_'></a>[üßæ TF-IDF Model](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSe7foICW7QO"
      },
      "source": [
        "#### <a id='toc4_1_1_1_'></a>[TF-IDF Model Initialization](#toc0_)\n",
        "\n",
        "The first baseline used for the evaluation consists of a **TF-IDF** model (**no machine learning used**).\n",
        "\n",
        "The model, a simple vectorizer built using the `Scikit Learn` library, computes the **TF-IDF scores** for each word and each document in the corpus and stores them in the `tf_idf_matrix` matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Cy7y1JnW7QO"
      },
      "outputs": [],
      "source": [
        "# Document IDs\n",
        "doc_ids = list(docs_dict.keys())\n",
        "# Document texts\n",
        "doc_texts = [docs_dict[doc_id]['text'].lower() for doc_id in doc_ids]\n",
        "# Remove empty documents from the list (and their respective IDs)\n",
        "doc_ids, doc_texts = zip(*[(doc_id, doc_text) for doc_id,\n",
        "                           doc_text in zip(doc_ids, doc_texts) if len(doc_text) > 0])\n",
        "\n",
        "# Get the TF-IDF vectorizer\n",
        "tf_idf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "# Fit the vectorizer on the document texts, computing the TF-IDF matrix (an [n_docs]x[vocab_size] matrix with the TF*IDF score value for each word in each document)\n",
        "tf_idf_matrix = tf_idf_vectorizer.fit_transform(doc_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ydFDPewW7QO"
      },
      "source": [
        "#### <a id='toc4_1_1_2_'></a>[TF-IDF Model Evaluation](#toc0_)\n",
        "\n",
        "We compute the **Mean Average Precision** (over `MAP_N` queries, each considering a precision at **K** defined by `MAP_K`) and the **Recall at K** (with **K** defined by `RECALL_K`) to **evaluate the TF-IDF model's performance**.\n",
        "\n",
        "We then **print the results** of the evaluation of both metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dD75V7bnW7QO"
      },
      "outputs": [],
      "source": [
        "if EVALUATE_MODELS:\n",
        "    print(\"Evaluating the TF-IDF model...\")\n",
        "    tf_idf_map_k = evaluation.compute_mean_average_precision_at_k(\n",
        "        MODEL_TYPES.TF_IDF, queries_dict, docs_dict,\n",
        "        k_documents=MAP_K, n_queries=MAP_N,\n",
        "        print_debug=PRINT_EVALUATION_DEBUG,\n",
        "        # Keyword arguments for the TF-IDF model\n",
        "        vectorizer=tf_idf_vectorizer, tfidf_matrix=tf_idf_matrix)\n",
        "    # Evaluate the TF-IDF model (compute the Recall@K)\n",
        "    tf_idf_recall_k = evaluation.compute_recall_at_k(\n",
        "        MODEL_TYPES.TF_IDF, queries_dict, docs_dict,\n",
        "        k_documents=RECALL_K,\n",
        "        print_debug=PRINT_EVALUATION_DEBUG,\n",
        "        # Keyword arguments for the TF-IDF model\n",
        "        vectorizer=tf_idf_vectorizer, tfidf_matrix=tf_idf_matrix)\n",
        "    # Print the evaluation results\n",
        "    print_model_evaluation_results(tf_idf_map_k, tf_idf_recall_k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uFkWwxTW7QO"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoL6s0klW7QO"
      },
      "source": [
        "<a id=\"4\"></a>\n",
        "# <a id='toc5_'></a>[üîù Word2Vec Model](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAHPx0bMW7QP"
      },
      "source": [
        "#### <a id='toc5_1_1_1_'></a>[Word2Vec Model Evaluation](#toc0_)\n",
        "\n",
        "We evaluate the `Word2Vec` model initialized in section \"[Word2Vec Model Initialization](#toc3_2_)\" and trained in section \"[Dictionaries Creation & Word2Vec Model Training](#toc3_3_)\" (to generate documents and queries embeddings) by computing the **Mean Average Precision** (over `MAP_N` queries, each considering a precision at **K** of `MAP_K`) and the **Recall at K** (with **K** defined by `RECALL_K`) to **evaluate the Word2Vec model's performance**.\n",
        "\n",
        "Both metrics are computed by using the trained **Word2Vec model** to generate a vector embedding for the given queries and for all the documents in the corpus, and then calculating the **cosine similarity** between the query embedding and the document embeddings to generate the top `K` most relevant documents.\n",
        "\n",
        "We therefore employ an **index-then-retrieve** approach, which is significantly slower (in the document retrieval phase) than the approach taken for the final **Seq2Seq transformer** model.\n",
        "\n",
        "If the model was not already trained (e.g. in case of documents and queries dictionaries being loaded from local files instead of being generated at runtime), we also **train the model** on the documents corpus.\n",
        "\n",
        "We then **print the results** of the evaluation of both metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsuJiezsW7QP"
      },
      "outputs": [],
      "source": [
        "# Check if the word2vec model needs to be trained\n",
        "if not word2vec_model.get_is_trained():\n",
        "    # Train the Word2Vec model on the documents corpus or load it from the checkpoint file\n",
        "    load_or_train_word2vec_model()\n",
        "\n",
        "# Use just the Word2Vec model (with which the embeddings were computed) to compute the similarity scores between the queries and the documents\n",
        "if EVALUATE_MODELS:\n",
        "    print(\"Computing the similarity scores between the queries and the documents using the Word2Vec model...\")\n",
        "    word2vec_map_k = evaluation.compute_mean_average_precision_at_k(\n",
        "        MODEL_TYPES.WORD2VEC, queries_dict, docs_dict,\n",
        "        k_documents=MAP_K, n_queries=MAP_N,\n",
        "        print_debug=PRINT_EVALUATION_DEBUG)\n",
        "    word2vec_recall_k = evaluation.compute_recall_at_k(\n",
        "        MODEL_TYPES.WORD2VEC, queries_dict, docs_dict,\n",
        "        k_documents=RECALL_K,\n",
        "        print_debug=PRINT_EVALUATION_DEBUG)\n",
        "    print_model_evaluation_results(word2vec_map_k, word2vec_recall_k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXtORS1HW7QP"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u83ZoWCYW7QP"
      },
      "source": [
        "<a id=\"5\"></a>\n",
        "# <a id='toc6_'></a>[üë¨ Siamese Network with Triplet Loss](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-6pFiRrW7QP"
      },
      "source": [
        "#### <a id='toc6_1_1_1_'></a>[Siamese Network Model Initialization](#toc0_)\n",
        "\n",
        "We initialize a **Siamese Netork model with Triplet Loss** to act as a third baseline for the evaluation of the finial **Seq2Seq transformer** model.\n",
        "\n",
        "The **hyperparameters** of the model are defined in the `siamese_network_args` dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VF5M-IiXW7QP"
      },
      "outputs": [],
      "source": [
        "# SiameseNetwork model's args\n",
        "siamese_network_args = {\n",
        "    \"input_size\": VECTOR_EMBEDDINGS_SIZE,\n",
        "    \"output_size\": SIAMESE_EMBEDDINGS_SIZE,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"margin\": 1.0,\n",
        "    \"dropout\": 0.0,\n",
        "    \"activation_function\": \"ReLU\"\n",
        "}\n",
        "\n",
        "# Create the Siamese Network model with Triplet Loss\n",
        "siamese_network_model = models.SiameseNetwork(**siamese_network_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZumzPk5AW7QP"
      },
      "source": [
        "#### <a id='toc6_1_1_2_'></a>[Siamese Network Dataset Creation](#toc0_)\n",
        "\n",
        "We create a **dataset** to then train and evaluate the **Siamese Network model** using the `SiameseNetworkDataset` class.\n",
        "\n",
        "The dataset consists of **triplets** of **anchor**, **positive** and **negative** samples, where:\n",
        "- **anchor** is a query id;\n",
        "- **positive** is a document ID of a document that is relevant to the corresponding query;\n",
        "- **negative** is a document ID of a document that is **NOT** relevant to the corresponding query.\n",
        "\n",
        "We also plot some **dataset triplet** examples to visualize the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ST3FodEW7QP"
      },
      "outputs": [],
      "source": [
        "# Create the dataset for the Siamese Network model\n",
        "#   The dataset will be a list of triplets (anchor_query, positive_document, negative_document)\n",
        "siamese_triplets_dataset = datasets.SiameseNetworkDataset(\n",
        "    queries_dict, docs_dict,\n",
        "    dataset_file_path=DATA_FOLDER + \"/siamese_triplets_dataset.json\",\n",
        "    force_dataset_rebuild=FORCE_DICTIONARIES_REBUILD\n",
        ")\n",
        "\n",
        "# Print the number of triplets in the dataset\n",
        "print(\n",
        "    f\"Number of [query, document+, document-] triplets in the dataset: {len(siamese_triplets_dataset.triplets)}\")\n",
        "\n",
        "# Print an example of a triplet\n",
        "print_triplet_example = True\n",
        "if print_triplet_example:\n",
        "    print(\"Example of a triplet:\")\n",
        "    triplet_example = siamese_triplets_dataset.triplets[0]\n",
        "    print(\"  [query, document+, document-]: \", triplet_example)\n",
        "    # Print the text of the query, the positive document and the negative document\n",
        "    print(\"  Query text: \", queries_dict[triplet_example[0]][\"text\"])\n",
        "    print(\"  Positive document text: \",\n",
        "          docs_dict[triplet_example[1]][\"text\"])\n",
        "    print(\"  Negative document text: \",\n",
        "          docs_dict[triplet_example[2]][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR1o0I3cW7QQ"
      },
      "source": [
        "#### <a id='toc6_1_1_3_'></a>[Siamese Network Model Training](#toc0_)\n",
        "\n",
        "We train the **Siamese Network model** using the `train_siamese` function of the custom `training` module.\n",
        "\n",
        "At the end of training, if a `WANDB_API_KEY` was provided (and thus the **Weights & Biases logger** was used), we also load the Weights & Biases dashboard to **plot the training results**.\n",
        "\n",
        "If the model was **already trained** and **saved to a checkpoint file** in the `MODEL_CHECKPOINTS_FILES`, and if the `LOAD_MODELS_CHECKPOINTS` is set to `true`, we **load the model** from the checkpoint file instead of training it again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYYsHqx-W7QQ"
      },
      "outputs": [],
      "source": [
        "# Model's checkpoint file path\n",
        "model_checkpoint_file = MODELS_FOLDER + \"/\" + \\\n",
        "    MODEL_CHECKPOINTS_FILES[MODEL_TYPES.SIAMESE_NETWORK]\n",
        "\n",
        "# Train or load the Siamese Network model\n",
        "if LOAD_MODELS_CHECKPOINTS and os.path.exists(model_checkpoint_file):\n",
        "    # Load the Siamese Network model from the checkpoint file\n",
        "    print(\n",
        "        \"A checkpoint file for the Siamese Network model exists, loading the model...\")\n",
        "    siamese_network_model = models.SiameseNetwork.load_from_checkpoint(\n",
        "        model_checkpoint_file, **siamese_network_args)\n",
        "    print(\"Checkpoint for the Siamese Network model loaded.\")\n",
        "else:\n",
        "    # Create a new logger for the Siamese Network model\n",
        "    siamese_wandb_logger = None\n",
        "    if wandb_api is not None:\n",
        "        siamese_wandb_logger = WandbLogger(\n",
        "            log_model=\"all\", project=wandb_project, name=\"Siamese Network\")\n",
        "    # Train the Siamese Network model\n",
        "    siamese_training_infos = training.train_siamese(\n",
        "        siamese_dataset=siamese_triplets_dataset,\n",
        "        siamese_model=siamese_network_model,\n",
        "        max_epochs=20,\n",
        "        batch_size=512,\n",
        "        split_ratio=0.8,\n",
        "        logger=siamese_wandb_logger,\n",
        "        save_path=model_checkpoint_file\n",
        "    )\n",
        "    # Show the W&B run's dashboard\n",
        "    if wandb_api is not None:\n",
        "        print(\"Training results for the Siamese Network model:\")\n",
        "        run_id = siamese_training_infos[\"run_id\"]\n",
        "        run_object: wandb_run.Run = wandb_api.run(\n",
        "            f\"{wandb_entity}/{wandb_project}/{run_id}\")\n",
        "        run_object.display(height=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPAALUY8W7QQ"
      },
      "source": [
        "#### <a id='toc6_1_1_4_'></a>[Siamese Network Model Evaluation](#toc0_)\n",
        "\n",
        "\n",
        "We compute the **Mean Average Precision** (over `MAP_N` queries, each considering a precision at **K** of `MAP_K`) and the **Recall at K** (with **K** defined by `RECALL_K`) to **evaluate the Siamese Network model's performance**.\n",
        "\n",
        "Once again, as for the Word2Vec model (see section \"[Word2Vec Model Initialization](#toc3_3_)\"), both metrics are computed by using the trained **Siamese Network model**, which in turn takes as input the vector embeddings for documents and queries computed using the **Word2Vec model**, to generate a vector embedding of size `SIAMESE_EMBEDDINGS_SIZE` for the given queries and for all the documents in the corpus, and then calculating the **cosine similarity** between the query embedding and the document embeddings to generate the top `K` most relevant documents.\n",
        "\n",
        "We therefore employ the same **index-then-retrieve** approach used for the Word2Vec model.\n",
        "\n",
        "We then **print the results** of the evaluation of both metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7MaWNM4W7QQ"
      },
      "outputs": [],
      "source": [
        "if EVALUATE_MODELS:\n",
        "    print(\"Evaluating the Siamese Network model...\")\n",
        "    siamese_net_map_k = evaluation.compute_mean_average_precision_at_k(\n",
        "        MODEL_TYPES.SIAMESE_NETWORK, queries_dict, docs_dict,\n",
        "        k_documents=MAP_K, n_queries=MAP_N,\n",
        "        print_debug=PRINT_EVALUATION_DEBUG,\n",
        "        # Keyword arguments for the Siamese Network model\n",
        "        model=siamese_network_model)\n",
        "    # Evaluate the Siamese Network model (compute the Recall@K)\n",
        "    siamese_net_recall_k = evaluation.compute_recall_at_k(\n",
        "        MODEL_TYPES.SIAMESE_NETWORK, queries_dict, docs_dict,\n",
        "        k_documents=RECALL_K,\n",
        "        print_debug=PRINT_EVALUATION_DEBUG,\n",
        "        # Keyword arguments for the Siamese Network model\n",
        "        model=siamese_network_model)\n",
        "    # Print the evaluation results\n",
        "    print_model_evaluation_results(siamese_net_map_k, siamese_net_recall_k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Df6N77vW7QQ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7wCjvELW7QQ"
      },
      "source": [
        "<a id=\"6\"></a>\n",
        "# <a id='toc7_'></a>[ü§ñ Seq2Seq Transformer Model (DSI approach)](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnQDUoQ0W7QQ"
      },
      "source": [
        "In this section we implement 3 possible versions of a **Seq2Seq transformer model** to act as the final model for the evaluation of the **Differentiable Search Index** approach.\n",
        "\n",
        "The **Seq2Seq transformer models** are trained to generate a **sorted list of document IDs** in response to a given **query**.\n",
        "\n",
        "The 3 transformer models, described in detail in the sub-sections below, are:\n",
        "\n",
        "1. **Seq2Seq Transformer Model using _teacher forcing_**: A Seq2Seq transformer model trained using only the **teacher forcing** technique, no auto-regressive decoding is used during training.\n",
        "\n",
        "   At inference time, instead, the model uses an **auto-regressive decoding** technique to generate the sorted list of document IDs.\n",
        "\n",
        "2. **Seq2Seq Transformer Model using _auto-regressive decoding_**: A Seq2Seq transformer model trained using only the **auto-regressive decoding** technique, no teacher forcing is used during training.\n",
        "\n",
        "   This model also uses the same **auto-regressive decoding** technique at inference time to generate the sorted list of document IDs.\n",
        "\n",
        "3. **Seq2Seq Transformer Model using _scheduled sampling_**: A Seq2Seq transformer model trained using the **scheduled sampling** technique, which consists of training the model using a mix of teacher forcing and auto-regressive decoding, by using tokens taken from either the ground truth or the model's own predictions during training, based on a probability defined by the `scheduled_sampling_decay` hyperparameter.\n",
        "\n",
        "   Once again, the model uses only the **auto-regressive decoding** technique at inference time to generate the sorted list of document IDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDXB73GWW7QQ"
      },
      "outputs": [],
      "source": [
        "# Compute the max length of the document IDS\n",
        "if REMAP_DOC_IDS:\n",
        "    # Doc IDs are remapped to a range [0, n_docs-1], so the max length depends on the number of documents\n",
        "    doc_ids_max_length = int(math.floor(math.log10(len(docs_dict))) + 1)\n",
        "else:\n",
        "    # We calculate the max length of the doc IDs as the length of the longest doc ID\n",
        "    doc_ids_max_length = max([len(doc_id) for doc_id in docs_dict])\n",
        "\n",
        "# Number of output tokens for the encoded document IDs (the 10 digits [0-9] plus the special tokens, i.e. end of sequence, padding, start of sequence)\n",
        "output_tokens = 10 + 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvLUPndLW7QR"
      },
      "source": [
        "#### <a id='toc7_1_1_1_'></a>[Transformer Datasets Creation](#toc0_)\n",
        "\n",
        "We create the **datasets** to train and evaluate the **Seq2Seq transformer models** using the `Seq2SeqDataset` class.\n",
        "\n",
        "Two different datasets are created:\n",
        "\n",
        "- **Indexing Dataset**: A dataset to train the model for the **indexing task**, in which the model learns to generate document IDs starting from **documents' text embeddings** as source sequences.\n",
        "\n",
        "   Items of the dataset have the form **`(encoded_document, encoded_doc_id)`** where `encoded_document` is the tokenized version of the document's text (i.e. a **vector of word token IDs** in the tokenizer's vocabulary), computed using a pretrained **BERT** model, and `encoded_doc_id` is a tokenized version of the document's ID in the documents dictionary, computed using an ad-hoc tokenizer which maps each digit of the document ID to an index (which is the same as the digit itself), and adds a special padding token, a special start-of-sequence token, and a special end-of-sequence token.\n",
        "\n",
        "- **Retrieval Dataset**: A dataset to train the model for the **retrieval task**, in which the model learns to generate document IDs starting from **queries' text embeddings** as source sequences.\n",
        "\n",
        "   Items of the dataset have the form **`(encoded_query, encoded_doc_id)`** where `encoded_query` is the tokenized version of the query's text, computed using the same **BERT** model, and `encoded_doc_id` is the tokenized version of the document's ID in the documents dictionary, computed using the same ad-hoc tokenizer used for the **Indexing Dataset**.\n",
        "\n",
        "Both datasets are shared among the 3 transformer models for training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ka5Kpp-kW7QR"
      },
      "outputs": [],
      "source": [
        "# Get the datasets for the transformer model (datasets are shared between the 3 transformer models)\n",
        "transformer_indexing_dataset = datasets.TransformerIndexingDataset(\n",
        "    documents=docs_dict,\n",
        "    doc_id_max_length=doc_ids_max_length,\n",
        "    doc_max_length=TRANSFORMER_DOCUMENT_MAX_TOKENS,\n",
        "    dataset_file_path=DATA_FOLDER + \"/transformer_indexing_dataset.json\",\n",
        "    force_dataset_rebuild=FORCE_DICTIONARIES_REBUILD)\n",
        "transformer_retrieval_dataset = datasets.TransformerRetrievalDataset(\n",
        "    documents=docs_dict, queries=queries_dict,\n",
        "    doc_id_max_length=doc_ids_max_length,\n",
        "    query_max_length=TRANSFORMER_QUERY_MAX_TOKENS,\n",
        "    dataset_file_path=DATA_FOLDER + \"/transformer_retrieval_dataset.json\",\n",
        "    force_dataset_rebuild=FORCE_DICTIONARIES_REBUILD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA70gEhTW7QR"
      },
      "source": [
        "We print some **examples** of the **Indexing Dataset** and the **Retrieval Dataset** to visualize the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1vRlp5yW7QR"
      },
      "outputs": [],
      "source": [
        "# Print some examples of the Transformers datasets\n",
        "print_dataset_examples = True\n",
        "if print_dataset_examples:\n",
        "    print(\"Example of a <encoded_doc, encoded_doc_id> pair:\")\n",
        "    encoded_doc, encoded_doc_id = transformer_indexing_dataset[random.randint(\n",
        "        0, len(transformer_indexing_dataset) - 1)]\n",
        "    print(\"  Encoded document:\\n  \", encoded_doc)\n",
        "    print(\"  Encoded document ID:\\n  \", encoded_doc_id)\n",
        "    print(\"Example of a <encoded_query, encoded_doc_id> pair:\")\n",
        "    encoded_query, encoded_relevant_doc_id = transformer_retrieval_dataset[random.randint(\n",
        "        0, len(transformer_retrieval_dataset) - 1)]\n",
        "    print(\"  Encoded query:\\n  \", encoded_query)\n",
        "    print(\"  Encoded relevant document ID:\\n  \", encoded_relevant_doc_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGieHp0UW7QR"
      },
      "source": [
        "#### <a id='toc7_1_1_2_'></a>[Transformer Models Initialization & Training](#toc0_)\n",
        "\n",
        "We create an auxiliary function to **initialize, train and evaluate** the **3 Seq2Seq transformer models**.\n",
        "\n",
        "The function considers the same constants and hyperparameters for all the Transformer model's different versions, with the exception of the parameters used for training.\n",
        "\n",
        "Each of the 3 Transformer models uses a different training approach (as explained in the introduction of this section), and then plots the training metrics (loss, accuracy, etc...) using the **Weights & Biases logger** for both training phases.\n",
        "\n",
        "Note that at inference time, in order to retrieve the top `K` most relevant documents, all the different Transformer models use the same **auto-regressive decoding** technique (generating document IDs' tokens one at a time, conditioning the generation of the next token on the previously generated tokens).\n",
        "\n",
        "For each model, we first train for the **indexing task**, using the `TransformerIndexingDataset` dataset, then train for the **retrieval task**, using the `TransformerRetrievalDataset` dataset (defined above).\n",
        "\n",
        "Before starting to train each model for the retrieval task, the retrieval dataset is split into a **training**, **validation** and **test** set: the latter is then used to evaluate the models' performance, thus for computing the **Mean Average Precision** and the **Recall at K**.\n",
        "\n",
        "For **computing the evaluation metrics**, we use a similar approach to the one used for the **Word2Vec** and **Siamese Network** models, but with the difference than in this case, while **training the model requires longer** than the previously described models (used as baselines), the **retrieval phase is significantly faster**, as the model directly optputs document IDs relevant to the query given as input, thus not requiring to compute the cosine similarity between the query and the entire documents corpus to find the top `K` most relevant documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1eWfs5AW7QR"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_dsi_transformer(transformer_type):\n",
        "    ''' Auxiliary function to train (or load checkpoints), show training results, and evaluate the transformer model of the given type '''\n",
        "\n",
        "    # args to pass to the dsi transformer model\n",
        "    use_scheduled_sampling_decay = \\\n",
        "        transformer_type == models.DSITransformer.TRANSFORMER_TYPES.SCHEDULED_SAMPLING_TRANSFORMER\n",
        "    dsi_transformer_args = {\n",
        "        \"tokens_in_vocabulary\": transformer_indexing_dataset.tokenizer.vocab_size,\n",
        "        \"embeddings_size\": TRANSFORMER_EMBEDDINGS_SIZE,\n",
        "        \"target_tokens\": output_tokens,\n",
        "        \"transformer_heads\": 4,\n",
        "        \"layers\": 3,\n",
        "        \"dropout\": 0.2,\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"batch_size\": 512,\n",
        "        \"transformer_type\": transformer_type,\n",
        "        \"scheduled_sampling_decay\": 0.01 if use_scheduled_sampling_decay else 0.0\n",
        "    }\n",
        "\n",
        "    # Initialize transformer model (using scheduled sampling)\n",
        "    transformer_model = models.DSITransformer(\n",
        "        **dsi_transformer_args)\n",
        "\n",
        "    # Model's checkpoint path\n",
        "    model_type_string = \"\"\n",
        "    if transformer_type == models.DSITransformer.TRANSFORMER_TYPES.SCHEDULED_SAMPLING_TRANSFORMER:\n",
        "        model_type_string = \"scheduled_sampling\"\n",
        "    elif transformer_type == models.DSITransformer.TRANSFORMER_TYPES.AUTOREGRESSIVE_TRANSFORMER:\n",
        "        model_type_string = \"autoregressive\"\n",
        "    elif transformer_type == models.DSITransformer.TRANSFORMER_TYPES.TEACHER_FORCINIG_TRANSFORMER:\n",
        "        model_type_string = \"teacher_forcing\"\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f\"Invalid transformer type: {transformer_type}\")\n",
        "    model_checkpoint_file = MODELS_FOLDER + \"/\" + \\\n",
        "        model_type_string + \"_\" + \\\n",
        "        MODEL_CHECKPOINTS_FILES[MODEL_TYPES.DSI_TRANSFORMER]\n",
        "\n",
        "    # Train the model or load its saved checkpoint\n",
        "    transformer_retrieval_test_set = None\n",
        "    transformer_retrieval_test_set_file = DATA_FOLDER + \\\n",
        "        f\"/{model_type_string}_transformer_retrieval_test_set.json\"\n",
        "    if LOAD_MODELS_CHECKPOINTS and os.path.exists(model_checkpoint_file):\n",
        "        # Load the saved models checkpoint\n",
        "        print(\"A checkpoint for the model exist, loading the saved model checkpoint...\")\n",
        "        transformer_model = models.DSITransformer.load_from_checkpoint(\n",
        "            model_checkpoint_file, **dsi_transformer_args)\n",
        "        print(\"Model checkpoint loaded.\")\n",
        "        # Load the transformer retrieval test set from the JSON file\n",
        "        print(\"Loading the transformer retrieval test set from the JSON file...\")\n",
        "        with open(transformer_retrieval_test_set_file, \"r\") as transformer_retrieval_test_set_file:\n",
        "            transformer_retrieval_test_set = json.load(\n",
        "                transformer_retrieval_test_set_file)\n",
        "        print(\"Transformer retrieval test set loaded.\")\n",
        "    else:\n",
        "        # Create 2 loggers for the transformer model (one for the indexing task and one for the retrieval task)\n",
        "        transformer_loggers = None\n",
        "        if wandb_api is not None:\n",
        "            transformer_wandb_logger_indexing = WandbLogger(\n",
        "                log_model=\"all\", project=wandb_project, name=transformer_type + \" (Indexing)\")\n",
        "            transformer_wandb_logger_retrieval = WandbLogger(\n",
        "                log_model=\"all\", project=wandb_project, name=transformer_type + \" (Retrieval)\")\n",
        "            transformer_loggers = [transformer_wandb_logger_indexing,\n",
        "                                   transformer_wandb_logger_retrieval]\n",
        "        # Train the transformer model (with scheduled sampling) for the indexing task\n",
        "        transformer_training_infos = training.train_transformer(\n",
        "            transformer_indexing_dataset=transformer_indexing_dataset,\n",
        "            transformer_retrieval_dataset=transformer_retrieval_dataset,\n",
        "            transformer_model=transformer_model,\n",
        "            max_epochs_list=[250, 150],\n",
        "            batch_size=transformer_model.hparams.batch_size,\n",
        "            indexing_split_ratios=(1.0, 0.0),\n",
        "            retrieval_split_ratios=(0.9, 0.05, 0.05),\n",
        "            logger=transformer_loggers,\n",
        "            save_path=model_checkpoint_file\n",
        "        )\n",
        "        # Show the wandb training run's dashboard\n",
        "        if wandb_api is not None:\n",
        "            indexing_run_id = transformer_training_infos[\"run_ids\"][\"indexing\"]\n",
        "            if indexing_run_id is not None:\n",
        "                print(f\"Indexing training results for the {transformer_type} model:\")\n",
        "                indexing_run_object: wandb_run.Run = wandb_api.run(\n",
        "                    f\"{wandb_entity}/{wandb_project}/{indexing_run_id}\")\n",
        "                indexing_run_object.display(height=1000)\n",
        "            retrieval_run_id = transformer_training_infos[\"run_ids\"][\"retrieval\"]\n",
        "            if retrieval_run_id is not None:\n",
        "                print(f\"Retrieval training results for the {transformer_type} model:\")\n",
        "                retrieval_run_object: wandb_run.Run = wandb_api.run(\n",
        "                    f\"{wandb_entity}/{wandb_project}/{retrieval_run_id}\")\n",
        "                retrieval_run_object.display(height=1000)\n",
        "        # Save the generated transformer retrieval test set to the JSON file\n",
        "        print(\"Saving the transformer retrieval test set to the JSON file...\")\n",
        "        retrieval_test_dataset = transformer_training_infos[\"retrieval\"][\"test\"]\n",
        "        transformer_retrieval_test_set = {\n",
        "            \"encoded_queries\": [],\n",
        "            \"encoded_doc_ids\": []\n",
        "        }\n",
        "        retrieval_test_dataset_length = retrieval_test_dataset.__len__()\n",
        "        for i in range(retrieval_test_dataset_length):\n",
        "            encoded_query, doc_id = retrieval_test_dataset.__getitem__(i)\n",
        "            transformer_retrieval_test_set[\"encoded_queries\"].append(\n",
        "                encoded_query.tolist())\n",
        "            transformer_retrieval_test_set[\"encoded_doc_ids\"].append(\n",
        "                doc_id.tolist())\n",
        "        with open(transformer_retrieval_test_set_file, \"w\") as transformer_retrieval_test_set_file:\n",
        "            json.dump(transformer_retrieval_test_set,\n",
        "                      transformer_retrieval_test_set_file)\n",
        "\n",
        "    # Evaluate the transformer model (for the retrieval task)\n",
        "    if EVALUATE_MODELS:\n",
        "        transformer_retrieval_map_k = evaluation.compute_mean_average_precision_at_k(\n",
        "            MODEL_TYPES.DSI_TRANSFORMER, queries_dict, docs_dict,\n",
        "            k_documents=MAP_K, n_queries=MAP_N,\n",
        "            print_debug=PRINT_EVALUATION_DEBUG,\n",
        "            # Keyword arguments for the Transformer model\n",
        "            model=transformer_model, retrieval_dataset=transformer_retrieval_dataset, retrieval_test_set=transformer_retrieval_test_set)\n",
        "        transformer_retrieval_recall_k = evaluation.compute_recall_at_k(\n",
        "            MODEL_TYPES.DSI_TRANSFORMER, queries_dict, docs_dict,\n",
        "            k_documents=RECALL_K,\n",
        "            print_debug=PRINT_EVALUATION_DEBUG,\n",
        "            # Keyword arguments for the Transformer model\n",
        "            model=transformer_model, retrieval_dataset=transformer_retrieval_dataset, retrieval_test_set=transformer_retrieval_test_set)\n",
        "        print_model_evaluation_results(transformer_retrieval_map_k,\n",
        "                                       transformer_retrieval_recall_k)\n",
        "\n",
        "    return transformer_model, transformer_retrieval_map_k, transformer_retrieval_recall_k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PDW9h2rW7QS"
      },
      "source": [
        "<a id=\"6_1\"></a>\n",
        "\n",
        "## <a id='toc7_2_'></a>[Teacher Forcing Seq2Seq Transformer Model](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXT3FVMfW7QS"
      },
      "source": [
        "The first version of the **Seq2Seq transformer model** is trained using only the **teacher forcing** technique, no auto-regressive decoding is used during training.\n",
        "\n",
        "This means that, during training, the model is fed with the **ground truth** document IDs as target sequences, and is therefore trained to generate the correct document IDs given both the query and the ground truth document IDs as input.\n",
        "\n",
        "At inference time, the model uses the usual **auto-regressive decoding** technique to generate token logits (instead of probabilities, as no softmax is applied to the output of the Transformer model) for all possible document IDs tokens.\n",
        "\n",
        "We **train the model** and then, if a `WANDB_API_KEY` was provided, we also load the **Weights & Biases** dashboard to **plot the training results** for both the indexing and retrieval tasks (in this order).\n",
        "\n",
        "After training, we then evaluate the Transformer model by computing the usual **Mean Average Precision** (over `MAP_N` queries, each considering a precision at **K** of `MAP_K`) and the **Recall at K** (with **K** defined by `RECALL_K`) to **teacher forcing Seq2Seq transformer model**.\n",
        "\n",
        "We then **print the results** of the evaluation of both metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jML1dB2lW7QS"
      },
      "outputs": [],
      "source": [
        "# Train and evaluate the transformer model using only teacher forcing\n",
        "teacher_forcing_transformer, teacher_forcing_transformer_map_k, teacher_forcing_transformer_recall_k = \\\n",
        "    train_and_evaluate_dsi_transformer(\n",
        "        models.DSITransformer.TRANSFORMER_TYPES.TEACHER_FORCINIG_TRANSFORMER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX-fk0KCW7QS"
      },
      "source": [
        "<a id=\"6_1\"></a>\n",
        "\n",
        "## <a id='toc7_3_'></a>[Autoregressive Seq2Seq Transformer Model](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca6TF9hLW7QS"
      },
      "source": [
        "The second version of the **Seq2Seq transformer model** is trained using an **auto-regressive decoding** technique, no teacher forcing is used during training.\n",
        "\n",
        "This means that, during training, the model learns to generate the correct document IDs by relying only on its own predictions, and not on the ground truth document IDs.\n",
        "\n",
        "The same **auto-regressive decoding** technique is also used at inferencing time to generate token logits (instead of probabilities, as no softmax is applied to the output of the Transformer model) for all possible document IDs tokens.\n",
        "\n",
        "We **train the model** and then, if a `WANDB_API_KEY` was provided, we also load the **Weights & Biases** dashboard to **plot the training results** for both the indexing and retrieval tasks (in this order).\n",
        "\n",
        "After training, we then evaluate the Transformer model by computing the usual **Mean Average Precision** (over `MAP_N` queries, each considering a precision at **K** of `MAP_K`) and the **Recall at K** (with **K** defined by `RECALL_K`) to **teacher forcing Seq2Seq transformer model**.\n",
        "\n",
        "We then **print the results** of the evaluation of both metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dy6D3A0IW7QS"
      },
      "outputs": [],
      "source": [
        "# Train and evaluate the transformer model using ony an autoregressive approach\n",
        "autoregressive_transformer, autoregressive_transformer_map_k, autoregressive_transformer_recall_k = \\\n",
        "    train_and_evaluate_dsi_transformer(\n",
        "        models.DSITransformer.TRANSFORMER_TYPES.AUTOREGRESSIVE_TRANSFORMER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y309DaEeW7QS"
      },
      "source": [
        "<a id=\"6_3\"></a>\n",
        "\n",
        "## <a id='toc7_4_'></a>[Scheduled Sampling Seq2Seq Transformer Model](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiE8BNHEW7QS"
      },
      "source": [
        "The final version of the **Seq2Seq transformer model** is trained using the **scheduled sampling** technique, which consists of training the model using a mix of teacher forcing and auto-regressive decoding, by using tokens taken from either the ground truth or the model's own predictions during training, based on a certain probability: this probability is initially set to 1.0 and then decays linearly over time, after each training epoch, by a factor defined by the `scheduled_sampling_decay` hyperparameter.\n",
        "\n",
        "During training, at each new token generation, the model decides whether to use the ground truth token (teacher forcing) or the previously generated token (autoregression) as input for the next token generation, based on the current probability.\n",
        "\n",
        "At inference time, only the **auto-regressive decoding** approach is used to generate token logits (instead of probabilities, as no softmax is applied to the output of the Transformer model) for all possible document IDs tokens.\n",
        "\n",
        "We **train the model** and then, if a `WANDB_API_KEY` was provided, we also load the **Weights & Biases** dashboard to **plot the training results** for both the indexing and retrieval tasks (in this order).\n",
        "\n",
        "After training, we then evaluate the Transformer model by computing the usual **Mean Average Precision** (over `MAP_N` queries, each considering a precision at **K** of `MAP_K`) and the **Recall at K** (with **K** defined by `RECALL_K`) to **teacher forcing Seq2Seq transformer model**.\n",
        "\n",
        "We then **print the results** of the evaluation of both metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQUIkSinW7QS"
      },
      "outputs": [],
      "source": [
        "# Train and evaluate the transformer model using scheduled sampling\n",
        "scheduled_sampling_transformer, scheduled_sampling_transformer_map_k, scheduled_sampling_transformer_recall_k = \\\n",
        "    train_and_evaluate_dsi_transformer(\n",
        "        models.DSITransformer.TRANSFORMER_TYPES.SCHEDULED_SAMPLING_TRANSFORMER)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}